{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source folder is: /Users/aureliennioche/Documents/PythonProjects/ProspecTonk/data/source\n",
      "The figure folder is: /Users/aureliennioche/Documents/PythonProjects/ProspecTonk/fig\n",
      "The backup folder is: /Users/aureliennioche/Documents/PythonProjects/ProspecTonk/data/backup\n"
     ]
    }
   ],
   "source": [
    "FIG_FOLDER = 'fig'\n",
    "SOURCE_FOLDER = os.path.join('data', 'source')\n",
    "BACKUP_FOLDER = os.path.join('data', 'backup')\n",
    "print(f\"The source folder is: {os.path.abspath(SOURCE_FOLDER)}\")\n",
    "print(f\"The figure folder is: {os.path.abspath(FIG_FOLDER)}\")\n",
    "print(f\"The backup folder is: {os.path.abspath(BACKUP_FOLDER)}\")\n",
    "\n",
    "# Create folders\n",
    "for f in SOURCE_FOLDER, FIG_FOLDER, BACKUP_FOLDER:\n",
    "    os.makedirs(f, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    param_labels = ['distortion', 'precision', 'risk_aversion']\n",
    "    fit_bounds = [(0.2, 1.8), (0.1, 10.0), (-0.99, 0.99)]\n",
    "\n",
    "    def __init__(self, param):\n",
    "        self.distortion, self.precision, self.risk_aversion = param\n",
    "    \n",
    "    def p_choice(self, p0, x0, p1, x1, c, *args, **kwargs):\n",
    "\n",
    "        p = self.p(p0=p0, x0=x0, p1=p1, x1=x1)\n",
    "        return p[int(c)]\n",
    "\n",
    "    @classmethod\n",
    "    def softmax(cls, v, precision):\n",
    "        return expit(v/precision)\n",
    "\n",
    "    @staticmethod\n",
    "    def u(x, risk_aversion):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            raise Exception\n",
    "        else:\n",
    "            if x >= 0:\n",
    "                return x ** (1-risk_aversion)\n",
    "            else:\n",
    "                return - np.abs(x) ** (1 + risk_aversion)\n",
    "    \n",
    "    @classmethod\n",
    "    def pi(cls, p, alpha):\n",
    "        if isinstance(p, np.ndarray):\n",
    "            to_return = np.zeros(p.shape)\n",
    "            unq_zero = p != 0\n",
    "            to_return[unq_zero] = np.exp(-(-np.log(p)) ** alpha)\n",
    "            return to_return\n",
    "        else:\n",
    "            if p == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return np.exp(-(-np.log(p)) ** alpha)\n",
    "\n",
    "    def p(self, p0, x0, p1, x1):\n",
    "\n",
    "        v0 = self.pi(p0, self.distortion) * self.u(x0, self.risk_aversion)\n",
    "        v1 = self.pi(p1, self.distortion) * self.u(x1, self.risk_aversion)\n",
    "        p = np.zeros(2)\n",
    "        p[0] = self.softmax(v0-v1, self.precision)\n",
    "        p[1] = 1 - p[0]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = np.finfo(float).eps\n",
    "\n",
    "def objective(param, model, data):\n",
    "    # Since we will look for the minimum, \n",
    "    # let's return -LLS instead of LLS\n",
    "\n",
    "    inst = model(param=param)\n",
    "\n",
    "    n = len(data)\n",
    "    ll = np.zeros(n)\n",
    "\n",
    "    for i, (_, row) in enumerate(data.iterrows()):\n",
    "        pi = inst.p_choice(**row)\n",
    "        ll[i] = np.log(pi + EPS)\n",
    "\n",
    "    return -ll.sum()\n",
    "\n",
    "def optimize(model, data):\n",
    "\n",
    "    # Define an init guess\n",
    "    init_guess = [(b[1] - b[0])/2 for b in model.fit_bounds]\n",
    "\n",
    "    # Run the optimizer\n",
    "    res = scipy.optimize.minimize(\n",
    "        fun=objective,\n",
    "        x0=init_guess,\n",
    "        bounds=model.fit_bounds,\n",
    "        args=(model, data))\n",
    "\n",
    "    # Make sure that the optimizer ended up with success\n",
    "    assert res.success\n",
    "\n",
    "    # Get the best param and best value from the \n",
    "    best_param = res.x\n",
    "    best_value = res.fun\n",
    "\n",
    "    return best_param, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = np.finfo(float).eps\n",
    "\n",
    "def fast_objective(param, model, data):\n",
    "    # Since we will look for the minimum, \n",
    "    # let's return -LLS instead of LLS\n",
    "    \n",
    "    distortion, precision, risk_aversion = param\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    sp0 = np.exp(-(-np.log(data.p0.values)) ** distortion)\n",
    "    sp1 = np.exp(-(-np.log(data.p1.values)) ** distortion)\n",
    "    \n",
    "    if np.all(data.x0.values >= 0) and np.all(data.x1.values >= 0):\n",
    "        su0 = data.x0.values ** (1 - risk_aversion)\n",
    "        su1 = data.x1.values ** (1 - risk_aversion)\n",
    "        \n",
    "    elif np.all(data.x0.values <= 0) and np.all(data.x1.values <= 0):\n",
    "        su0 = - np.abs(data.x0.values) ** (1 + risk_aversion)\n",
    "        su1 = - np.abs(data.x1.values) ** (1 + risk_aversion)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    v0 = sp0 * su0\n",
    "    v1 = sp1 * su1\n",
    "    \n",
    "    delta = v0 - v1\n",
    "    \n",
    "    p = np.zeros((2, n))\n",
    "    p[0] = expit(delta/precision)\n",
    "    p[1] = 1 - p[0]\n",
    "\n",
    "    lls = np.log(p[0, data.c.values==0] + EPS).sum()\n",
    "    lls += np.log(p[1, data.c.values==1] + EPS).sum()\n",
    "    return -lls\n",
    "\n",
    "def fast_optimize(model, data):\n",
    "\n",
    "    # Define an init guess\n",
    "    init_guess = [(b[1] - b[0])/2 for b in model.fit_bounds]\n",
    "\n",
    "    # Run the optimizer\n",
    "    res = scipy.optimize.minimize(\n",
    "        fun=fast_objective,\n",
    "        x0=init_guess,\n",
    "        bounds=model.fit_bounds,\n",
    "        args=(model, data))\n",
    "\n",
    "    # Make sure that the optimizer ended up with success\n",
    "    assert res.success\n",
    "\n",
    "    # Get the best param and best value from the \n",
    "    best_param = res.x\n",
    "    best_value = res.fun\n",
    "\n",
    "    return best_param, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stimuli(n_trial):\n",
    "    \n",
    "    possible_p = [0.25, 0.5, 0.75, 1]\n",
    "    possible_x = np.arange(1, 4)\n",
    "    p = np.array([np.random.choice(possible_p, size=2, replace=False) for _ in range(n_trial)])\n",
    "    p = np.sort(p, axis=-1)\n",
    "    x = np.array([np.random.choice(possible_x, size=2, replace=False) for _ in range(n_trial)])\n",
    "    x = -np.sort(-x, axis=-1)\n",
    "    return pd.DataFrame({\"p0\": p[:, 0], \"p1\": p[:, 1], \"x0\": x[:, 0], \"x1\": x[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(model, param, n_trial):\n",
    "    m = Model(param=param)\n",
    "    stim = generate_stimuli(n_trial=n_trial)\n",
    "    d = []\n",
    "    for t, (_, row) in enumerate(stim.iterrows()):\n",
    "        p = m.p(**row)\n",
    "        c = int(p[1] > np.random.random())\n",
    "        d.append({\"c\": c, **row})\n",
    "    data = pd.DataFrame(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_simulate(model, param, n_trial):\n",
    "\n",
    "    data = generate_stimuli(n_trial=n_trial)\n",
    "    \n",
    "    distortion, precision, risk_aversion = param\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    sp0 = np.exp(-(-np.log(data.p0.values)) ** distortion)\n",
    "    sp1 = np.exp(-(-np.log(data.p1.values)) ** distortion)\n",
    "    \n",
    "    su0 = data.x0.values ** (1 - risk_aversion)\n",
    "    su1 = data.x1.values ** (1 - risk_aversion)\n",
    "    \n",
    "    v0 = sp0 * su0\n",
    "    v1 = sp1 * su1\n",
    "    \n",
    "    delta = v0 - v1\n",
    "    \n",
    "    p = np.zeros((n, 2))\n",
    "    p[:, 0] = expit(delta/precision)\n",
    "    p[:, 1] = 1 - p[:, 0]\n",
    "    \n",
    "    c = np.zeros(n, dtype=int)\n",
    "    r = np.random.random(size=n)\n",
    "    c[:] = p[:, 1] > r\n",
    "    \n",
    "    data[\"c\"] = c\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
